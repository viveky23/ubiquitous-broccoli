{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_train_250 = \"../Human Experiments/Data/RL_trajectories_1000_250.pickle\"\n",
    "file_obj_train_250 = open(file_name_train_250, 'rb')\n",
    "trajectories_train_250 = pickle.load(file_obj_train_250)\n",
    "trajectories_train_250 = [list(ele) for ele in trajectories_train_250]\n",
    "\n",
    "file_name_test_250 = \"../Human Experiments/Data/Human_trajectories_test_250.pickle\"\n",
    "file_obj_test_250 = open(file_name_test_250, 'rb')\n",
    "trajectories_test_250 = pickle.load(file_obj_test_250)\n",
    "trajectories_test_250 = [list(ele) for ele in trajectories_test_250]\n",
    "\n",
    "file_name_train_500 = \"../Human Experiments/Data/RL_trajectories_1000_500.pickle\"\n",
    "file_obj_train_500 = open(file_name_train_500, 'rb')\n",
    "trajectories_train_500 = pickle.load(file_obj_train_500)\n",
    "trajectories_train_500 = [list(ele) for ele in trajectories_train_500]\n",
    "\n",
    "file_name_test_500 = \"../Human Experiments/Data/Human_trajectories_test_500.pickle\"\n",
    "file_obj_test_500 = open(file_name_test_500, 'rb')\n",
    "trajectories_test_500 = pickle.load(file_obj_test_500)\n",
    "trajectories_test_500 = [list(ele) for ele in trajectories_test_500]\n",
    "\n",
    "test_size = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policies(price_low_bound, price_upper_bound):\n",
    "    policies = []\n",
    "    for i in range(price_low_bound, price_upper_bound, 10):\n",
    "        for j in range(price_low_bound, price_upper_bound, 10):\n",
    "            for k in range(price_low_bound, price_upper_bound, 10):\n",
    "                for l in range(5):\n",
    "                    for m in range(5):\n",
    "                        for n in range(5):\n",
    "                            policy = (i, j, k, l, m, n)\n",
    "                            policies.append(policy)\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = generate_policies(150, 260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, data, best_buys_cost, best_buys_idx, get_final_output):\n",
    "    if(len(policy)!=6):\n",
    "        raise ValueError(\"Number of parameters in this policy is \"+ str(len(policy))+\" when it should be 6\")\n",
    "    bought = []\n",
    "    bought_cost_only = []\n",
    "    bought_idx_only = []\n",
    "    t1 = policy[0]\n",
    "    t2 = policy[1]\n",
    "    t3 = policy[2]\n",
    "    d1 = policy[3]\n",
    "    d2 = policy[4]\n",
    "    d3 = policy[5]\n",
    "    for trajectory in data:\n",
    "        for idx, cost in enumerate(trajectory):\n",
    "            local_cat = int(idx/5)\n",
    "            local_idx = int(idx%5)\n",
    "            if(idx == len(trajectory) - 1):\n",
    "                bought.append((cost, idx))\n",
    "                bought_cost_only.append(cost)\n",
    "                bought_idx_only.append(idx)\n",
    "                break\n",
    "            elif(local_cat == 0):\n",
    "                if(cost > t1 and local_idx <= d1):\n",
    "                    continue\n",
    "                else: \n",
    "                    bought.append((cost, idx))\n",
    "                    bought_cost_only.append(cost)\n",
    "                    bought_idx_only.append(idx)\n",
    "                    break\n",
    "            elif(local_cat == 1):\n",
    "                if(cost > t2 and local_idx <= d2):\n",
    "                    continue\n",
    "                else: \n",
    "                    bought.append((cost, idx))\n",
    "                    bought_cost_only.append(cost)\n",
    "                    bought_idx_only.append(idx)\n",
    "                    break\n",
    "            elif(local_cat == 2):\n",
    "                if(cost > t3 and local_idx <= d3):\n",
    "                    continue\n",
    "                else: \n",
    "                    bought.append((cost, idx))\n",
    "                    bought_cost_only.append(cost)\n",
    "                    bought_idx_only.append(idx)\n",
    "                    break\n",
    "    error = [a_i - b_i for a_i, b_i in zip(bought_cost_only, best_buys_cost)]\n",
    "    correct_stops = (np.equal(bought_idx_only, best_buys_idx)).astype(int)\n",
    "    correct_stops = sum(correct_stops)\n",
    "    avg_error = sum(error)/len(data)\n",
    "    if(get_final_output):\n",
    "        return bought, avg_error, correct_stops\n",
    "    return avg_error, correct_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cost\n",
    "best_buy_250_train = [min(ele) for ele in trajectories_train_250]\n",
    "best_buy_250_test = [min(ele) for ele in trajectories_test_250]\n",
    "best_buy_500_train = [min(ele) for ele in trajectories_train_500]\n",
    "best_buy_500_test = [min(ele) for ele in trajectories_test_500]\n",
    "\n",
    "##Indices\n",
    "best_buy_250_train_idx = [ele.index(min(ele)) for ele in trajectories_train_250]\n",
    "best_buy_250_test_idx = [ele.index(min(ele)) for ele in trajectories_test_250]\n",
    "best_buy_500_train_idx = [ele.index(min(ele)) for ele in trajectories_train_500]\n",
    "best_buy_500_test_idx = [ele.index(min(ele)) for ele in trajectories_test_500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.737, 524)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy((190, 210, 230, 4, 4, 3), trajectories_train_250, best_buy_250_train, best_buy_250_train_idx, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_search(price_low_bound, price_upper_bound, data, best_buys_cost, best_buys_idx):\n",
    "    all_errors = []\n",
    "    all_correct_stops = []\n",
    "    all_policies = generate_policies(price_low_bound, price_upper_bound)\n",
    "    for idx, policy in enumerate(all_policies):\n",
    "        policy_avg_error, correct_stops = evaluate_policy(policy, data, best_buys_cost, best_buys_idx, False)\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\rPolicies Evaluated: {}/{}\".format(idx+1, len(all_policies)))\n",
    "        all_errors.append(policy_avg_error)\n",
    "        all_correct_stops.append(correct_stops)\n",
    "    optimal_policy_by_avg_error = (min(all_errors), all_policies[all_errors.index(min(all_errors))])\n",
    "    optimal_policy_by_opt_stop = (max(all_correct_stops), all_policies[all_correct_stops.index(max(all_correct_stops))])\n",
    "    return optimal_policy_by_avg_error, optimal_policy_by_opt_stop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Route 1 (250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies Evaluated: 27000/27000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11.59, (200, 210, 230, 4, 4, 3)), (582, (200, 210, 220, 4, 4, 3)))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy_250 = policy_search(180, 231, trajectories_train_250, best_buy_250_train, best_buy_250_train_idx)\n",
    "optimal_policy_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.716666666666667, 25)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_avg_250, avg_error, correct_stops = evaluate_policy((200, 210, 230, 4, 4, 3), \n",
    "                                                                 trajectories_test_250, \n",
    "                                                                 best_buy_250_test, \n",
    "                                                                 best_buy_250_test_idx, \n",
    "                                                                 True)\n",
    "avg_error, correct_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.483333333333334, 24)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_opt_stop_250, avg_error, correct_stops = evaluate_policy((200, 210, 220, 4, 4, 3), \n",
    "                                                                 trajectories_test_250, \n",
    "                                                                 best_buy_250_test, \n",
    "                                                                 best_buy_250_test_idx, \n",
    "                                                                 True)\n",
    "avg_error, correct_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Route 2 (500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies Evaluated: 343000/343000"
     ]
    }
   ],
   "source": [
    "optimal_policy_500 = policy_search(370, 501, trajectories_train_500, best_buy_500_train, best_buy_500_train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20.699, (410, 420, 460, 4, 4, 4)), (621, (400, 420, 430, 4, 4, 3)))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.333333333333332, 34)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_avg_500, avg_error, correct_stops = evaluate_policy((410, 420, 460, 4, 4, 4), \n",
    "                                                                 trajectories_test_500, \n",
    "                                                                 best_buy_500_test, \n",
    "                                                                 best_buy_500_test_idx, \n",
    "                                                                 True)\n",
    "avg_error, correct_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.9, 31)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result_opt_stop_500, avg_error, correct_stops = evaluate_policy((400, 420, 430, 4, 4, 3), \n",
    "                                                                 trajectories_test_500, \n",
    "                                                                 best_buy_500_test, \n",
    "                                                                 best_buy_500_test_idx, \n",
    "                                                                 True)\n",
    "avg_error, correct_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_three_split_avg = [final_result_avg_250 + final_result_avg_500]\n",
    "rl_three_split_opt_stop = [final_result_opt_stop_250 + final_result_opt_stop_500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(obj, filename):\n",
    "    final_file_object = open(filename, 'wb')\n",
    "    pickle.dump(obj, final_file_object)\n",
    "    final_file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(rl_three_split_avg, \"../Human Experiments/Tests/rl_three_split_avg\")\n",
    "save_data(rl_three_split_opt_stop, \"../Human Experiments/Tests/rl_three_split_opt_stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
